import { and, inArray, sql } from "drizzle-orm";
import { drizzle } from "drizzle-orm/bun-sql";
import { schema } from "@/integrations/drizzle";
import { ReactiveResumeV4JSONImporter } from "@/integrations/import/reactive-resume-v4-json";
import { defaultResumeData } from "@/schema/resume/data";
import { generateId } from "@/utils/string";

// Types for the production database
type Visibility = "public" | "private";

interface ProductionResume {
	id: string;
	title: string;
	slug: string;
	data: unknown; // JSON data
	visibility: Visibility;
	locked: boolean;
	userId: string;
	createdAt: Date;
	updatedAt: Date;
}

interface ProductionStatistics {
	id: string;
	views: number;
	downloads: number;
	resumeId: string;
	createdAt: Date;
	updatedAt: Date;
}

const productionUrl = process.env.PRODUCTION_DATABASE_URL;
const localUrl = process.env.DATABASE_URL;

if (!productionUrl) throw new Error("PRODUCTION_DATABASE_URL is not set");
if (!localUrl) throw new Error("DATABASE_URL is not set");

const productionClient = new Bun.SQL({ url: productionUrl });
const localClient = new Bun.SQL({ url: localUrl });

// == Persistent mapping file path ==
const USER_ID_MAP_FILE = "./scripts/migration/user-id-map.json";

// You may tune this for your use case
const BATCH_SIZE = 10000;

async function loadUserIdMapFromFile(): Promise<Map<string, string>> {
	try {
		const file = Bun.file(USER_ID_MAP_FILE);
		if (await file.exists()) {
			const text = await file.text();
			const obj = JSON.parse(text);
			return new Map(Object.entries(obj));
		}
	} catch (e) {
		console.warn("‚ö†Ô∏è  Failed to load userIdMap from disk, continuing with empty map.", e);
	}
	return new Map<string, string>();
}

export async function migrateResumes() {
	const migrationStart = performance.now();
	console.log("‚åõ Starting resume migration...");

	// Connect to both databases
	const productionDb = drizzle({ client: productionClient, connection: productionUrl });
	const localDb = drizzle({ client: localClient, connection: localUrl, schema });

	let hasMore = true;
	let currentOffset = 0;

	// Load persistent userIdMap from file
	const userIdMap = await loadUserIdMapFromFile();

	// Track migration stats
	let resumesCreated = 0;
	let statisticsCreated = 0;
	let skipped = 0;
	let totalResumesProcessed = 0;
	let errors = 0;

	// Initialize the importer
	const importer = new ReactiveResumeV4JSONImporter();

	while (hasMore) {
		console.log(`üì• Fetching resumes batch from production database (OFFSET ${currentOffset})...`);

		const resumes = (await productionDb.execute(sql`
			SELECT id, title, slug, data, visibility, locked, "userId", "createdAt", "updatedAt"
			FROM "Resume"
			ORDER BY "id" ASC
			LIMIT ${BATCH_SIZE} OFFSET ${currentOffset}
		`)) as unknown as ProductionResume[];

		console.log(`üìã Found ${resumes.length} resumes in this batch.`);

		if (resumes.length === 0) {
			hasMore = false;
			break;
		}

		// Fetch statistics only for these resumes in this batch
		const resumeIds = resumes.map((r) => r.id);

		// Drizzle does not interpolate arrays, so we join and use a custom SQL string
		const resumeIdsForSql = resumeIds.map((id) => `'${id}'`).join(", ");

		const statistics = (await productionDb.execute(sql`
			SELECT id, views, downloads, "resumeId", "createdAt", "updatedAt"
			FROM "Statistics"
			WHERE "resumeId" IN (${sql.raw(resumeIdsForSql)})
		`)) as unknown as ProductionStatistics[];

		// Create a map of resumeId -> statistics for quick lookup
		const statisticsMap = new Map<string, ProductionStatistics>();
		for (const stat of statistics) {
			statisticsMap.set(stat.resumeId, stat);
		}

		// Filter out resumes where userId is not in userIdMap
		const resumesToProcess = resumes
			.map((resume) => {
				const newUserId = userIdMap.get(resume.userId);
				if (!newUserId) {
					skipped++;
					return null;
				}
				return { resume, newUserId };
			})
			.filter((item): item is NonNullable<typeof item> => item !== null);

		if (resumesToProcess.length === 0) {
			console.log(`‚è≠Ô∏è  All resumes in this batch have userIds not found in userIdMap.`);
			currentOffset += resumes.length;
			totalResumesProcessed += resumes.length;
			continue;
		}

		// Get unique userIds and bulk check if they exist in local database
		const uniqueUserIds = [...new Set(resumesToProcess.map((r) => r.newUserId))];
		const existingUsers = await localDb.select().from(schema.user).where(inArray(schema.user.id, uniqueUserIds));

		const existingUserIds = new Set(existingUsers.map((u) => u.id));

		// Filter out resumes where user doesn't exist
		const resumesWithValidUsers = resumesToProcess.filter(({ newUserId }) => {
			if (!existingUserIds.has(newUserId)) {
				skipped++;
				return false;
			}
			return true;
		});

		if (resumesWithValidUsers.length === 0) {
			console.log(`‚è≠Ô∏è  All resumes in this batch have userIds not found in local database.`);
			currentOffset += resumes.length;
			totalResumesProcessed += resumes.length;
			continue;
		}

		// Bulk check for existing resumes (by slug + userId)
		// We need to check each unique combination
		const slugUserIdPairs = resumesWithValidUsers.map(({ resume, newUserId }) => ({
			slug: resume.slug,
			userId: newUserId,
		}));

		// Get all unique slugs and userIds
		const uniqueSlugs = [...new Set(slugUserIdPairs.map((p) => p.slug))];
		const userIdsForSlugCheck = [...new Set(slugUserIdPairs.map((p) => p.userId))];

		// Fetch all existing resumes that match any of our slugs and userIds
		const existingResumes = await localDb
			.select()
			.from(schema.resume)
			.where(and(inArray(schema.resume.slug, uniqueSlugs), inArray(schema.resume.userId, userIdsForSlugCheck)));

		// Create a set of existing slug+userId combinations
		const existingResumeKeys = new Set(existingResumes.map((r) => `${r.slug}:${r.userId}`));

		// Filter out resumes that already exist
		const resumesToInsert = resumesWithValidUsers.filter(({ resume, newUserId }) => {
			const key = `${resume.slug}:${newUserId}`;
			if (existingResumeKeys.has(key)) {
				skipped++;
				return false;
			}
			return true;
		});

		if (resumesToInsert.length === 0) {
			console.log(`‚è≠Ô∏è  All resumes in this batch already exist in target DB.`);
			currentOffset += resumes.length;
			totalResumesProcessed += resumes.length;
			continue;
		}

		console.log(`üìù Preparing to bulk insert ${resumesToInsert.length} resumes...`);

		// Prepare bulk insert data
		const batchStart = performance.now();
		try {
			const resumesToInsertData = resumesToInsert.map(({ resume, newUserId }) => {
				// Transform the data using the V4 importer
				let transformedData = defaultResumeData;
				try {
					const dataJson = typeof resume.data === "string" ? resume.data : JSON.stringify(resume.data);
					transformedData = importer.parse(dataJson);
				} catch (error) {
					console.error(`‚ö†Ô∏è  Failed to parse resume data for resume ${resume.id}, using default data:`, error);
					// Use default data if parsing fails
					transformedData = defaultResumeData;
				}

				// Map visibility to isPublic (visibility === "public" -> isPublic = true)
				const isPublic = resume.visibility === "public";

				const newResumeId = generateId();

				return {
					resumeData: {
						id: newResumeId,
						name: resume.title,
						slug: resume.slug,
						tags: [], // Default empty array
						isPublic: isPublic,
						isLocked: resume.locked,
						password: null, // No password in old schema
						data: transformedData,
						userId: newUserId,
						createdAt: resume.createdAt,
						updatedAt: resume.updatedAt,
					},
					originalResumeId: resume.id,
					newResumeId: newResumeId,
				};
			});

			// Bulk insert resumes
			await localDb.insert(schema.resume).values(resumesToInsertData.map(({ resumeData }) => resumeData));
			resumesCreated += resumesToInsertData.length;

			// Prepare statistics for bulk insert
			const statisticsToInsert = resumesToInsertData
				.map(({ originalResumeId, newResumeId }) => {
					const resumeStatistics = statisticsMap.get(originalResumeId);
					if (!resumeStatistics) return null;

					return {
						id: generateId(),
						views: resumeStatistics.views,
						downloads: resumeStatistics.downloads,
						lastViewedAt: null, // Not available in old schema
						lastDownloadedAt: null, // Not available in old schema
						resumeId: newResumeId,
						createdAt: resumeStatistics.createdAt,
						updatedAt: resumeStatistics.updatedAt,
					};
				})
				.filter((stat): stat is NonNullable<typeof stat> => stat !== null);

			// Bulk insert statistics
			if (statisticsToInsert.length > 0) {
				await localDb.insert(schema.resumeStatistics).values(statisticsToInsert);
				statisticsCreated += statisticsToInsert.length;
			}

			const batchEnd = performance.now();
			const batchTimeMs = batchEnd - batchStart;
			console.log(
				`‚úÖ Bulk inserted ${resumesToInsertData.length} resumes in ${batchTimeMs.toFixed(1)} ms (avg ${(batchTimeMs / resumesToInsertData.length).toFixed(1)} ms/resume)`,
			);
		} catch (error) {
			console.error(`üö® Failed to bulk insert resumes batch:`, error);
			errors++;
			// Continue with next batch even if this one fails
		}

		currentOffset += resumes.length;
		totalResumesProcessed += resumes.length;
		console.log(`üì¶ Processed ${totalResumesProcessed} resumes so far...\n`);
	}

	const migrationEnd = performance.now();
	const migrationDurationMs = migrationEnd - migrationStart;

	console.log("\nüìä Migration Summary:");
	console.log(`   Resumes created: ${resumesCreated}`);
	console.log(`   Statistics created: ${statisticsCreated}`);
	console.log(`   Skipped (userId not found or already exist): ${skipped}`);
	console.log(`   Errors: ${errors}`);
	console.log(
		`‚è±Ô∏è  Total migration time: ${migrationDurationMs.toFixed(1)} ms (${(migrationDurationMs / 1000).toFixed(2)} seconds)`,
	);

	console.log("‚úÖ Resume migration complete!");
}

if (import.meta.main) {
	try {
		await migrateResumes();
	} finally {
		await productionClient.close();
		await localClient.close();
	}
}
